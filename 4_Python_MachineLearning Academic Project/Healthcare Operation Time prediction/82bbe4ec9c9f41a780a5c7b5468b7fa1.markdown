---
jupyter:
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.11.3
  nav_menu:
    height: 279px
    width: 309px
  nbformat: 4
  nbformat_minor: 4
  toc:
    nav_menu: {}
    number_sections: true
    sideBar: true
    skip_h1_title: false
    toc_cell: false
    toc_position: {}
    toc_section_display: block
    toc_window_display: false
---

::: {.cell .markdown}
**End-to-end Machine Learning project**
:::

::: {.cell .markdown}
```{=html}
<table align="left">
  <td>
    <a href="https://colab.research.google.com/github/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
  <td>
    <a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a>
  </td>
</table>
```
:::

::: {.cell .code execution_count="1"}
``` python
print("Welcome to Machine Learning!")
```

::: {.output .stream .stdout}
    Welcome to Machine Learning!
:::
:::

::: {.cell .markdown}
This project requires Python 3.7 or above:
:::

::: {.cell .code execution_count="2"}
``` python
import sys

assert sys.version_info >= (3, 7)
```
:::

::: {.cell .markdown}
It also requires Scikit-Learn ≥ 1.0.1:
:::

::: {.cell .code execution_count="3"}
``` python
from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")
```
:::

::: {.cell .markdown}
# Get the Data
:::

::: {.cell .markdown}
*The task is to predict the operation time in a plastic surgery
hostpital for each patient, given a number of features based on the
patient\'s individual circumstances.*

*Data dictionary is at the end of the notebook.*
:::

::: {.cell .markdown}
## Download the Data
:::

::: {.cell .code execution_count="4"}
``` python
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

df = pd.read_csv("C:/Users/Tony/Desktop/Code repos/patient.csv")
```
:::

::: {.cell .markdown}
## Take a Quick Look at the Data Structure
:::

::: {.cell .code execution_count="5"}
``` python
df.head()
```

::: {.output .execute_result execution_count="5"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SEX</th>
      <th>CPT</th>
      <th>ELECTSURG</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>HYPERMED</th>
      <th>DIALYSIS</th>
      <th>DIAB</th>
      <th>SMOKE</th>
      <th>WNDCLAS</th>
      <th>ASACLAS</th>
      <th>OPTIME</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>female</td>
      <td>19364</td>
      <td>Yes</td>
      <td>46</td>
      <td>68</td>
      <td>211</td>
      <td>32.08211</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>1</td>
      <td>2</td>
      <td>8.8833</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>female</td>
      <td>19364</td>
      <td>Yes</td>
      <td>51</td>
      <td>62</td>
      <td>112</td>
      <td>20.48486</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>1</td>
      <td>1</td>
      <td>8.4500</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>female</td>
      <td>19364</td>
      <td>Yes</td>
      <td>48</td>
      <td>66</td>
      <td>110</td>
      <td>17.75428</td>
      <td>Yes</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>1</td>
      <td>3</td>
      <td>11.9333</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>female</td>
      <td>19364</td>
      <td>Yes</td>
      <td>42</td>
      <td>63</td>
      <td>222</td>
      <td>39.32513</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>3</td>
      <td>2</td>
      <td>10.6833</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>female</td>
      <td>19364</td>
      <td>Yes</td>
      <td>35</td>
      <td>64</td>
      <td>113</td>
      <td>19.39621</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>No</td>
      <td>1</td>
      <td>2</td>
      <td>5.7500</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="6"}
``` python
df.info()
```

::: {.output .stream .stdout}
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 6148 entries, 0 to 6147
    Data columns (total 18 columns):
     #   Column     Non-Null Count  Dtype  
    ---  ------     --------------  -----  
     0   SEX        6148 non-null   object 
     1   CPT        6148 non-null   int64  
     2   ELECTSURG  6148 non-null   object 
     3   AGE        6148 non-null   int64  
     4   HEIGHT     6148 non-null   int64  
     5   WEIGHT     6148 non-null   int64  
     6   BMI        6148 non-null   float64
     7   HYPERMED   6148 non-null   object 
     8   DIALYSIS   6148 non-null   object 
     9   DIAB       6148 non-null   object 
     10  SMOKE      6148 non-null   object 
     11  WNDCLAS    6148 non-null   int64  
     12  ASACLAS    6148 non-null   int64  
     13  OPTIME     6148 non-null   float64
     14  Plastic    6148 non-null   int64  
     15  After2014  6148 non-null   int64  
     16  URR        6148 non-null   int64  
     17  SSI        6148 non-null   int64  
    dtypes: float64(2), int64(10), object(6)
    memory usage: 864.7+ KB
:::
:::

::: {.cell .code execution_count="7"}
``` python
df.describe()
```

::: {.output .execute_result execution_count="7"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CPT</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>WNDCLAS</th>
      <th>ASACLAS</th>
      <th>OPTIME</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.00000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
      <td>6148.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>18774.277489</td>
      <td>51.397528</td>
      <td>64.470885</td>
      <td>172.151919</td>
      <td>29.151391</td>
      <td>1.28985</td>
      <td>2.371015</td>
      <td>8.026773</td>
      <td>0.943396</td>
      <td>0.748699</td>
      <td>0.053513</td>
      <td>0.086858</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1424.910077</td>
      <td>11.582654</td>
      <td>3.427176</td>
      <td>38.132150</td>
      <td>6.262224</td>
      <td>0.60116</td>
      <td>0.592331</td>
      <td>2.948908</td>
      <td>0.231103</td>
      <td>0.433797</td>
      <td>0.225073</td>
      <td>0.281649</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15756.000000</td>
      <td>9.000000</td>
      <td>39.000000</td>
      <td>50.000000</td>
      <td>8.320351</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>19364.000000</td>
      <td>44.000000</td>
      <td>62.000000</td>
      <td>146.000000</td>
      <td>24.809010</td>
      <td>1.00000</td>
      <td>2.000000</td>
      <td>6.083300</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>19364.000000</td>
      <td>51.000000</td>
      <td>64.000000</td>
      <td>169.000000</td>
      <td>28.568260</td>
      <td>1.00000</td>
      <td>2.000000</td>
      <td>7.908350</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>19364.000000</td>
      <td>59.000000</td>
      <td>67.000000</td>
      <td>194.000000</td>
      <td>32.809920</td>
      <td>1.00000</td>
      <td>3.000000</td>
      <td>9.816700</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42894.000000</td>
      <td>96.000000</td>
      <td>79.000000</td>
      <td>387.000000</td>
      <td>83.273250</td>
      <td>4.00000</td>
      <td>4.000000</td>
      <td>23.050000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .markdown}
The following cell is not shown either in the book. It creates the
`images/end_to_end_project` folder (if it doesn\'t already exist), and
it defines the `save_fig()` function which is used through this notebook
to save the figures in high-res for the book.
:::

::: {.cell .code execution_count="8"}
``` python
# extra code – code to save the figures as high-res PNGs for the book

IMAGES_PATH = Path() / "images" / "end_to_end_project"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```
:::

::: {.cell .code execution_count="9"}
``` python
import matplotlib.pyplot as plt

# extra code – the next 5 lines define the default font sizes
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

df.hist(bins=50, figsize=(12, 8))
save_fig("attribute_histogram_plots")  # extra code
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/ca5fd1e0da80fca9a27333bdf58159de0f970584.png)
:::
:::

::: {.cell .code execution_count="10"}
``` python
import matplotlib.pyplot as plt
import seaborn as sns

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

cat_features = df[['SEX',
'CPT',
'ELECTSURG',
'HYPERMED',
'DIALYSIS',
'DIAB',
'SMOKE'
]]

# Create a grid of subplots
num_rows = 3
num_cols = 3
fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 12))  # Adjust the figure size as needed

for i, col in enumerate(cat_features):
    row = i // num_cols
    col_idx = i % num_cols
    sns.countplot(y=col, data=df, ax=axes[row, col_idx])
    axes[row, col_idx].set_title(f"Distribution of {col}")

plt.tight_layout()  # This helps to fit everything neatly
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/f55921a809a66a725bb8932773953acdc0a5779a.png)
:::
:::

::: {.cell .markdown}
We see that there is no null values in this dataset. Here OPTIME is our
target column. Categorical variables dominate the features columns.
:::

::: {.cell .markdown}
## Create a Test Set
:::

::: {.cell .code execution_count="11"}
``` python
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```
:::

::: {.cell .code execution_count="12"}
``` python
train_set, test_set = shuffle_and_split_data(df, 0.2)
len(train_set)
```

::: {.output .execute_result execution_count="12"}
    4919
:::
:::

::: {.cell .code execution_count="13"}
``` python
len(test_set)
```

::: {.output .execute_result execution_count="13"}
    1229
:::
:::

::: {.cell .markdown}
To ensure that this notebook\'s outputs remain the same every time we
run it, we need to set the random seed:
:::

::: {.cell .code execution_count="14"}
``` python
np.random.seed(42)
```
:::

::: {.cell .code execution_count="15"}
``` python
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```
:::

::: {.cell .code execution_count="16"}
``` python
df_with_id = df.reset_index()  # adds an `index` column
train_set, test_set = split_data_with_id_hash(df_with_id, 0.2, "index")
```
:::

::: {.cell .code execution_count="17"}
``` python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(df_with_id, test_size=0.2, random_state=42)
```
:::

::: {.cell .markdown}
Let\'s compare the traditional train_test_split vs the
StratifiedShuffleSplit based on the BMI
:::

::: {.cell .code execution_count="18"}
``` python
df["BMI_cat"] = pd.cut(df["BMI"],
                               bins=[0,24, 28, 32, 37., np.inf],
                               labels=[1, 2, 3, 4, 5])
```
:::

::: {.cell .code execution_count="19"}
``` python
df["BMI_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.xlabel("BMI category")
plt.ylabel("Number of patients")
save_fig("BMI_cat_bar_plot")  # extra code
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/6a5a9ef7fd05be5bcb82fd8f9c865a878a3f0169.png)
:::
:::

::: {.cell .code execution_count="20"}
``` python
from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []
for train_index, test_index in splitter.split(df, df["BMI_cat"]):
    strat_train_set_n = df.iloc[train_index]
    strat_test_set_n = df.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])
```
:::

::: {.cell .code execution_count="21"}
``` python
strat_train_set, strat_test_set = strat_splits[0]
```
:::

::: {.cell .markdown}
It\'s much shorter to get a single stratified split:
:::

::: {.cell .code execution_count="22"}
``` python
strat_train_set, strat_test_set = train_test_split(
    df, test_size=0.2, stratify=df["BMI_cat"], random_state=42)
```
:::

::: {.cell .code execution_count="23"}
``` python
strat_test_set["BMI_cat"].value_counts() / len(strat_test_set)
```

::: {.output .execute_result execution_count="23"}
    2    0.259350
    3    0.243902
    1    0.204065
    4    0.191870
    5    0.100813
    Name: BMI_cat, dtype: float64
:::
:::

::: {.cell .code execution_count="24"}
``` python
# extra code – computes the data for Figure 2–10

def BMI_cat_proportions(data):
    return data["BMI_cat"].value_counts() / len(data)

train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)

compare_props = pd.DataFrame({
    "Overall %": BMI_cat_proportions(df),
    "Stratified %": BMI_cat_proportions(strat_test_set),
    "Random %": BMI_cat_proportions(test_set),
}).sort_index()
compare_props.index.name = "BMI Category"
compare_props["Strat. Error %"] = (compare_props["Stratified %"] /
                                   compare_props["Overall %"] - 1)
compare_props["Rand. Error %"] = (compare_props["Random %"] /
                                  compare_props["Overall %"] - 1)
(compare_props * 100).round(2)
```

::: {.output .execute_result execution_count="24"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Overall %</th>
      <th>Stratified %</th>
      <th>Random %</th>
      <th>Strat. Error %</th>
      <th>Rand. Error %</th>
    </tr>
    <tr>
      <th>BMI Category</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>20.38</td>
      <td>20.41</td>
      <td>20.33</td>
      <td>0.13</td>
      <td>-0.27</td>
    </tr>
    <tr>
      <th>2</th>
      <td>25.93</td>
      <td>25.93</td>
      <td>24.23</td>
      <td>0.03</td>
      <td>-6.55</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24.41</td>
      <td>24.39</td>
      <td>24.31</td>
      <td>-0.10</td>
      <td>-0.43</td>
    </tr>
    <tr>
      <th>4</th>
      <td>19.16</td>
      <td>19.19</td>
      <td>19.11</td>
      <td>0.14</td>
      <td>-0.29</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10.12</td>
      <td>10.08</td>
      <td>12.03</td>
      <td>-0.35</td>
      <td>18.93</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="25"}
``` python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("BMI_cat", axis=1, inplace=True)
```
:::

::: {.cell .markdown}
# Discover and Visualize the Data to Gain Insights
:::

::: {.cell .code execution_count="26"}
``` python
df = strat_train_set.copy()
```
:::

::: {.cell .markdown}
## Visualizing OPTIME VS BMI and AGE
:::

::: {.cell .code execution_count="27"}
``` python
df.plot(kind="scatter", x="HEIGHT", y="WEIGHT", grid=True, alpha=0.2)
save_fig("better_visualization_plot")  # extra code
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/abcf1cf08ca1502b66cd0d45cb516d2224172602.png)
:::
:::

::: {.cell .code execution_count="28"}
``` python
df['transformed_OPTIME'] = df['OPTIME'] ** 2

df.plot(kind="scatter", x="HEIGHT", y="WEIGHT", grid=True,
             s=(df["AGE"]**2)/60, label="AGE",
             c="transformed_OPTIME", cmap="jet", colorbar=True,
             legend=True, sharex=False, figsize=(10, 7))
save_fig("patients_scatterplot")  # extra code
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/667d118102e14139c9228b42d71df2735c1a1b4a.png)
:::
:::

::: {.cell .code execution_count="29"}
``` python
df = df.drop("transformed_OPTIME", axis=1)
```
:::

::: {.cell .markdown}
## Looking for Correlations
:::

::: {.cell .markdown}
Note: since Pandas 2.0.0, the `numeric_only` argument defaults to
`False`, so we need to set it explicitly to True to avoid an error.
:::

::: {.cell .code execution_count="30"}
``` python
corr_matrix = df.corr()
```

::: {.output .stream .stderr}
    C:\Users\Tony\AppData\Local\Temp\ipykernel_28760\3147368345.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
      corr_matrix = df.corr()
:::
:::

::: {.cell .code execution_count="31"}
``` python
corr_matrix["OPTIME"].sort_values(ascending=False)
```

::: {.output .execute_result execution_count="31"}
    OPTIME       1.000000
    URR          0.064044
    SSI          0.059559
    WNDCLAS      0.008459
    CPT          0.003056
    AGE         -0.004428
    HEIGHT      -0.011320
    ASACLAS     -0.013312
    BMI         -0.013772
    WEIGHT      -0.020764
    Plastic     -0.068498
    After2014   -0.094090
    Name: OPTIME, dtype: float64
:::
:::

::: {.cell .code execution_count="32"}
``` python
from pandas.plotting import scatter_matrix

attributes = ["AGE", "HEIGHT", "WEIGHT",
              "BMI"]
scatter_matrix(df[attributes], figsize=(12, 8))
save_fig("scatter_matrix_plot")  # extra code
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/9b95cab6f1126cd2dc7d9cec7a9cc7e16018e258.png)
:::
:::

::: {.cell .code execution_count="33"}
``` python
import matplotlib.pyplot as plt

# Sample pairs of continuous variables
# Replace these with your actual pairs of variables
pairs_of_variables = [('BMI', 'OPTIME'), ('AGE', 'OPTIME')]

# Set up the subplot grid
n_rows, n_cols = 1, 2
fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 10))  # Adjust the figure size as needed

# Flatten the array of axes for easy iteration
axs = axs.flatten()

# Loop through the pairs of variables and create scatter plots
for i, (x_var, y_var) in enumerate(pairs_of_variables):
    axs[i].scatter(df[x_var], df[y_var])
    axs[i].set_xlabel(x_var)
    axs[i].set_ylabel(y_var)
    axs[i].set_title(f'{x_var} vs {y_var}')

# Hide any unused subplots
for j in range(i + 1, n_rows * n_cols):
    axs[j].set_visible(False)

# Adjust layout for better spacing
plt.tight_layout()

# Display the plot
plt.show()

```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/b11faf88ac7be03ecf3e4189db70cce29260a4b3.png)
:::
:::

::: {.cell .code execution_count="34"}
``` python
import matplotlib.pyplot as plt
import seaborn as sns

binary_variables = ['SEX',
'CPT',
'ELECTSURG',
'HYPERMED',
'DIALYSIS',
'DIAB',
'SMOKE',
'WNDCLAS',
'ASACLAS',
'Plastic',
'After2014',
'URR',
'SSI'
]  # Replace with your variables

# Set up the subplot grid
n_rows, n_cols = 7, 2
fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 50))  # Adjust the figure size as needed

# Flatten the array of axes for easy iteration
axs = axs.flatten()

# Loop through the binary variables and create violin plots
for i, var in enumerate(binary_variables):
    sns.violinplot(x=var, y='OPTIME', data=df, ax=axs[i])  # Replace 'continuous_variable' with your variable
    axs[i].set_title(var)

# Hide any unused subplots
for j in range(i + 1, n_rows * n_cols):
    axs[j].set_visible(False)

# Adjust layout for better spacing
plt.tight_layout()

# Display the plot
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/347b08b98ce810c93325a57dc11493859289d6c9.png)
:::
:::

::: {.cell .markdown}
# Prepare the Data for Machine Learning Algorithms
:::

::: {.cell .code execution_count="35"}
``` python
#for column ELECTRSURG, there is only one row with value unknow, so we convert it to NO.
df = strat_train_set.copy()
df['ELECTSURG']=df['ELECTSURG'].replace("Unknown","No")
# For some other columns with only binary values , convert to dummy variables.
df['SEX']=df['SEX'].replace({'female': 1, 'male': 0})
df['ELECTSURG']=df['ELECTSURG'].replace({'Yes': 1, 'No': 0})
df['HYPERMED']=df['HYPERMED'].replace({'Yes': 1, 'No': 0})
df['DIALYSIS']=df['DIALYSIS'].replace({'Yes': 1, 'No': 0})
df['SMOKE']=df['SMOKE'].replace({'Yes': 1, 'No': 0})
df['CPT']=df['CPT'].replace({42894: 26551, 26553: 26551})
#in the case for DIAB, we assign the value based on its described seriousness. 
df['DIAB']=df['DIAB'].replace({'NON-INSULIN': 1, 'No': 0, 'INSULIN': 2})
```
:::

::: {.cell .markdown}
One hot encoding for all the other category variables.
:::

::: {.cell .code execution_count="36"}
``` python
columns_to_encode = ['CPT',
                     'WNDCLAS',
                     'ASACLAS']  
df_encoded = pd.get_dummies(df, columns=columns_to_encode)

df = df_encoded.copy()
```
:::

::: {.cell .code execution_count="37"}
``` python
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
df.describe()
```

::: {.output .execute_result execution_count="37"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SEX</th>
      <th>ELECTSURG</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>HYPERMED</th>
      <th>DIALYSIS</th>
      <th>DIAB</th>
      <th>SMOKE</th>
      <th>OPTIME</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
      <th>CPT_15756</th>
      <th>CPT_15757</th>
      <th>CPT_15758</th>
      <th>CPT_15842</th>
      <th>CPT_19364</th>
      <th>CPT_19368</th>
      <th>CPT_20955</th>
      <th>CPT_20962</th>
      <th>CPT_26551</th>
      <th>WNDCLAS_1</th>
      <th>WNDCLAS_2</th>
      <th>WNDCLAS_3</th>
      <th>WNDCLAS_4</th>
      <th>ASACLAS_1</th>
      <th>ASACLAS_2</th>
      <th>ASACLAS_3</th>
      <th>ASACLAS_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.00000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.00000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
      <td>4918.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.903619</td>
      <td>0.970720</td>
      <td>51.408906</td>
      <td>64.462180</td>
      <td>172.088857</td>
      <td>29.143478</td>
      <td>0.250915</td>
      <td>0.001017</td>
      <td>0.084791</td>
      <td>0.105937</td>
      <td>8.030592</td>
      <td>0.942253</td>
      <td>0.746238</td>
      <td>0.053884</td>
      <td>0.087434</td>
      <td>0.073810</td>
      <td>0.05673</td>
      <td>0.037617</td>
      <td>0.001423</td>
      <td>0.796869</td>
      <td>0.027247</td>
      <td>0.003050</td>
      <td>0.002237</td>
      <td>0.001017</td>
      <td>0.767588</td>
      <td>0.190118</td>
      <td>0.02379</td>
      <td>0.018503</td>
      <td>0.051647</td>
      <td>0.535990</td>
      <td>0.404026</td>
      <td>0.008337</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.295143</td>
      <td>0.168608</td>
      <td>11.547276</td>
      <td>3.429776</td>
      <td>38.202074</td>
      <td>6.258986</td>
      <td>0.433584</td>
      <td>0.031872</td>
      <td>0.337971</td>
      <td>0.307789</td>
      <td>2.950521</td>
      <td>0.233288</td>
      <td>0.435207</td>
      <td>0.225811</td>
      <td>0.282499</td>
      <td>0.261489</td>
      <td>0.23135</td>
      <td>0.190287</td>
      <td>0.037704</td>
      <td>0.402370</td>
      <td>0.162818</td>
      <td>0.055148</td>
      <td>0.047245</td>
      <td>0.031872</td>
      <td>0.422413</td>
      <td>0.392434</td>
      <td>0.15241</td>
      <td>0.134777</td>
      <td>0.221336</td>
      <td>0.498754</td>
      <td>0.490752</td>
      <td>0.090934</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>9.000000</td>
      <td>43.000000</td>
      <td>50.000000</td>
      <td>8.320351</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>44.000000</td>
      <td>62.000000</td>
      <td>145.000000</td>
      <td>24.856000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6.083300</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>51.000000</td>
      <td>64.000000</td>
      <td>169.000000</td>
      <td>28.568260</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.916700</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>59.000000</td>
      <td>67.000000</td>
      <td>194.000000</td>
      <td>32.784740</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>9.816700</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>96.000000</td>
      <td>79.000000</td>
      <td>375.000000</td>
      <td>83.273250</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>23.050000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="38"}
``` python
df.head()
```

::: {.output .execute_result execution_count="38"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SEX</th>
      <th>ELECTSURG</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>HYPERMED</th>
      <th>DIALYSIS</th>
      <th>DIAB</th>
      <th>SMOKE</th>
      <th>OPTIME</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
      <th>CPT_15756</th>
      <th>CPT_15757</th>
      <th>CPT_15758</th>
      <th>CPT_15842</th>
      <th>CPT_19364</th>
      <th>CPT_19368</th>
      <th>CPT_20955</th>
      <th>CPT_20962</th>
      <th>CPT_26551</th>
      <th>WNDCLAS_1</th>
      <th>WNDCLAS_2</th>
      <th>WNDCLAS_3</th>
      <th>WNDCLAS_4</th>
      <th>ASACLAS_1</th>
      <th>ASACLAS_2</th>
      <th>ASACLAS_3</th>
      <th>ASACLAS_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3479</th>
      <td>1</td>
      <td>1</td>
      <td>41</td>
      <td>72</td>
      <td>160</td>
      <td>21.69968</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>8.6833</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3191</th>
      <td>1</td>
      <td>1</td>
      <td>62</td>
      <td>65</td>
      <td>141</td>
      <td>23.46339</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>10.9167</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1</td>
      <td>1</td>
      <td>33</td>
      <td>63</td>
      <td>187</td>
      <td>33.12523</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6.6833</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5707</th>
      <td>0</td>
      <td>1</td>
      <td>45</td>
      <td>73</td>
      <td>323</td>
      <td>42.61428</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>7.8333</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5553</th>
      <td>1</td>
      <td>1</td>
      <td>39</td>
      <td>57</td>
      <td>92</td>
      <td>19.90840</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8.2167</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .markdown}
## Experimenting with Attribute Combinations
:::

::: {.cell .code execution_count="39"}
``` python
df["DIAB_SMOKE"] = df["DIAB"] * df["SMOKE"]
df["DIALYSIS_SMOKE"] = df["DIALYSIS"] * df["SMOKE"]
df["BMI_SMOKE"] = df["BMI"] * df["SMOKE"]
df["HYPERMED_SMOKE"] = df["HYPERMED"] * df["SMOKE"]
df["ELECTSURG_PLASTIC"] = df["ELECTSURG"] * df["Plastic"]
```
:::

::: {.cell .code execution_count="40"}
``` python
corr_matrix = df.corr(numeric_only=True)
corr_matrix["OPTIME"].sort_values(ascending=False)
```

::: {.output .execute_result execution_count="40"}
    OPTIME               1.000000
    URR                  0.064044
    SSI                  0.059559
    SEX                  0.027224
    WNDCLAS_4            0.024233
    ASACLAS_2            0.020144
    CPT_19364            0.017336
    HYPERMED             0.017076
    ASACLAS_4            0.017002
    HYPERMED_SMOKE       0.013009
    CPT_15757            0.008049
    DIAB                 0.005519
    WNDCLAS_1           -0.000408
    DIAB_SMOKE          -0.001235
    ASACLAS_1           -0.001340
    CPT_20955           -0.002636
    WNDCLAS_2           -0.003839
    AGE                 -0.004428
    ELECTSURG           -0.005299
    DIALYSIS            -0.007792
    CPT_15842           -0.008283
    CPT_15758           -0.008836
    CPT_15756           -0.009197
    WNDCLAS_3           -0.010415
    HEIGHT              -0.011320
    BMI                 -0.013772
    CPT_26551           -0.014064
    SMOKE               -0.014065
    CPT_19368           -0.018133
    CPT_20962           -0.018898
    WEIGHT              -0.020764
    ASACLAS_3           -0.023019
    BMI_SMOKE           -0.025091
    ELECTSURG_PLASTIC   -0.058347
    Plastic             -0.068498
    After2014           -0.094090
    DIALYSIS_SMOKE            NaN
    Name: OPTIME, dtype: float64
:::
:::

::: {.cell .code execution_count="41"}
``` python
df =df.drop('DIALYSIS_SMOKE',axis = 1)
```
:::

::: {.cell .code execution_count="42"}
``` python
X = df.drop("OPTIME", axis=1)
df_labels = strat_train_set["OPTIME"].copy()
```
:::

::: {.cell .markdown}
## Data Cleaning
:::

::: {.cell .markdown}
Since there is no null value in this dataset, we will skip the filling
the null step.
:::

::: {.cell .markdown}
Now let\'s drop some outliers:
:::

::: {.cell .code execution_count="43"}
``` python
from sklearn.ensemble import IsolationForest

isolation_forest = IsolationForest(random_state=42)
outlier_pred = isolation_forest.fit_predict(X)
```
:::

::: {.cell .code execution_count="44"}
``` python
mean = np.mean(outlier_pred)
mean
```

::: {.output .execute_result execution_count="44"}
    0.7080113867425782
:::
:::

::: {.cell .markdown}
If you wanted to drop outliers, you would run the following code. In
this case, I decided to keep it for now
:::

::: {.cell .code execution_count="45"}
``` python
#df = df.iloc[outlier_pred == 1]
#df_labels = df_labels.iloc[outlier_pred == 1]
```
:::

::: {.cell .markdown}
## Feature Scaling
:::

::: {.cell .code execution_count="46"}
``` python
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
X_min_max_scaled = min_max_scaler.fit_transform(X)
```
:::

::: {.cell .code execution_count="47"}
``` python
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
X_std_scaled = std_scaler.fit_transform(X)
```
:::

::: {.cell .code execution_count="48"}
``` python
# extra code – this cell generates Figure 2–17
fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)
df["BMI"].hist(ax=axs[0], bins=50)
df["BMI"].apply(np.log).hist(ax=axs[1], bins=50)
axs[0].set_xlabel("BMI")
axs[1].set_xlabel("Log of BMI")
axs[0].set_ylabel("Number of patients")
save_fig("long_tail_plot")
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/d42c9cf0691e832e06331be74705be42a1735301.png)
:::
:::

::: {.cell .markdown}
What if we replace each value with its percentile?
:::

::: {.cell .code execution_count="49"}
``` python
# extra code – just shows that we get a uniform distribution
percentiles = [np.percentile(df["BMI"], p)
               for p in range(1, 100)]
flattened_median_income = pd.cut(df["BMI"],
                                 bins=[-np.inf] + percentiles + [np.inf],
                                 labels=range(1, 100 + 1))
flattened_median_income.hist(bins=50)
plt.xlabel("BMI percentile")
plt.ylabel("Number of patients")
plt.show()
# Note: incomes below the 1st percentile are labeled 1, and incomes above the
# 99th percentile are labeled 100. This is why the distribution below ranges
# from 1 to 100 (not 0 to 100).
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/7336a7bcf94cb795f06de2eb9e6c36ea1f7046f2.png)
:::
:::

::: {.cell .code execution_count="50"}
``` python
from sklearn.metrics.pairwise import rbf_kernel

BMI_35 = rbf_kernel(df[["BMI"]], [[22]], gamma=0.1)
```
:::

::: {.cell .code execution_count="51"}
``` python
# extra code – this cell generates Figure 2–18

ages = np.linspace(df["BMI"].min(),
                   df["BMI"].max(),
                   500).reshape(-1, 1)
gamma1 = 0.1
gamma2 = 0.03
rbf1 = rbf_kernel(ages, [[22]], gamma=gamma1)
rbf2 = rbf_kernel(ages, [[22]], gamma=gamma2)

fig, ax1 = plt.subplots()

ax1.set_xlabel("BMI")
ax1.set_ylabel("Number of patients")
ax1.hist(df["BMI"], bins=50)

ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis
color = "blue"
ax2.plot(ages, rbf1, color=color, label="gamma = 0.10")
ax2.plot(ages, rbf2, color=color, label="gamma = 0.03", linestyle="--")
ax2.tick_params(axis='y', labelcolor=color)
ax2.set_ylabel("BMI similarity", color=color)

plt.legend(loc="upper left")
save_fig("age_similarity_plot")
plt.show()
```

::: {.output .display_data}
![](vertopal_82bbe4ec9c9f41a780a5c7b5468b7fa1/a89fd31382439dd6ea89631ac57f3dd55b76dab1.png)
:::
:::

::: {.cell .code execution_count="52"}
``` python
from sklearn.linear_model import LinearRegression

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(df_labels.to_frame())

model = LinearRegression()
model.fit(df[["BMI"]], scaled_labels)
some_new_data = df[["BMI"]].iloc[:5]  # pretend this is new data

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)
```
:::

::: {.cell .code execution_count="53"}
``` python
predictions
```

::: {.output .execute_result execution_count="53"}
    array([[8.07891954],
           [8.06746886],
           [8.00474051],
           [7.94313399],
           [8.09054921]])
:::
:::

::: {.cell .code execution_count="54"}
``` python
from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())
model.fit(df[["BMI"]], df_labels)
predictions = model.predict(some_new_data)
```
:::

::: {.cell .code execution_count="55"}
``` python
predictions
```

::: {.output .execute_result execution_count="55"}
    array([8.07891954, 8.06746886, 8.00474051, 7.94313399, 8.09054921])
:::
:::

::: {.cell .markdown}
## Custom Transformers
:::

::: {.cell .markdown}
To create simple transformers:
:::

::: {.cell .code execution_count="56"}
``` python
from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_BMI = log_transformer.transform(df[["BMI"]])
```
:::

::: {.cell .code execution_count="57"}
``` python
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[22.]], gamma=0.1))
bmi_simil_22 = rbf_transformer.transform(df[["BMI"]])
```
:::

::: {.cell .code execution_count="58"}
``` python
bmi_simil_22
```

::: {.output .execute_result execution_count="58"}
    array([[9.91021341e-01],
           [8.07226461e-01],
           [4.21410108e-06],
           ...,
           [1.73159232e-04],
           [1.12813329e-02],
           [1.12813329e-02]])
:::
:::

::: {.cell .code execution_count="59"}
``` python
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
```

::: {.output .execute_result execution_count="59"}
    array([[0.5 ],
           [0.75]])
:::
:::

::: {.cell .code execution_count="60"}
``` python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # no *args or **kwargs!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # y is required even though we don't use it
        X = check_array(X)  # checks that X is an array with finite float values
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
        return self  # always return self!

    def transform(self, X):
        check_is_fitted(self)  # looks for learned attributes (with trailing _)
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
```
:::

::: {.cell .code execution_count="61"}
``` python
from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, n_init=10,
                              random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # always return self!

    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
```
:::

::: {.cell .markdown}
**Warning**:

-   There was a change in Scikit-Learn 1.3.0 which affected the random
    number generator for `KMeans` initialization. Therefore the results
    will be different than in the book if you use Scikit-Learn ≥ 1.3.
    That\'s not a problem as long as you don\'t expect the outputs to be
    perfectly identical.
-   Throughout this notebook, when `n_init` was not set when creating a
    `KMeans` estimator, I explicitly set it to `n_init=10` to avoid a
    warning about the fact that the default value for this
    hyperparameter will change from 10 to `"auto"` in Scikit-Learn 1.4.
:::

::: {.cell .markdown}
## Transformation Pipelines
:::

::: {.cell .markdown}
Now let\'s build a pipeline to preprocess the numerical attributes:
:::

::: {.cell .code execution_count="62"}
``` python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])
```
:::

::: {.cell .code execution_count="63"}
``` python
from sklearn.pipeline import make_pipeline

num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())
```
:::

::: {.cell .code execution_count="64"}
``` python
from sklearn import set_config

set_config(display='diagram')

num_pipeline
```

::: {.output .execute_result execution_count="64"}
```{=html}
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),
                (&#x27;standardscaler&#x27;, StandardScaler())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" ><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;simpleimputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),
                (&#x27;standardscaler&#x27;, StandardScaler())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">SimpleImputer</label><div class="sk-toggleable__content"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div>
```
:::
:::

::: {.cell .code execution_count="65"}
``` python
X.head()
```

::: {.output .execute_result execution_count="65"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SEX</th>
      <th>ELECTSURG</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>HYPERMED</th>
      <th>DIALYSIS</th>
      <th>DIAB</th>
      <th>SMOKE</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
      <th>CPT_15756</th>
      <th>CPT_15757</th>
      <th>CPT_15758</th>
      <th>CPT_15842</th>
      <th>CPT_19364</th>
      <th>CPT_19368</th>
      <th>CPT_20955</th>
      <th>CPT_20962</th>
      <th>CPT_26551</th>
      <th>WNDCLAS_1</th>
      <th>WNDCLAS_2</th>
      <th>WNDCLAS_3</th>
      <th>WNDCLAS_4</th>
      <th>ASACLAS_1</th>
      <th>ASACLAS_2</th>
      <th>ASACLAS_3</th>
      <th>ASACLAS_4</th>
      <th>DIAB_SMOKE</th>
      <th>BMI_SMOKE</th>
      <th>HYPERMED_SMOKE</th>
      <th>ELECTSURG_PLASTIC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3479</th>
      <td>1</td>
      <td>1</td>
      <td>41</td>
      <td>72</td>
      <td>160</td>
      <td>21.69968</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>21.69968</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3191</th>
      <td>1</td>
      <td>1</td>
      <td>62</td>
      <td>65</td>
      <td>141</td>
      <td>23.46339</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1</td>
      <td>1</td>
      <td>33</td>
      <td>63</td>
      <td>187</td>
      <td>33.12523</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5707</th>
      <td>0</td>
      <td>1</td>
      <td>45</td>
      <td>73</td>
      <td>323</td>
      <td>42.61428</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5553</th>
      <td>1</td>
      <td>1</td>
      <td>39</td>
      <td>57</td>
      <td>92</td>
      <td>19.90840</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="66"}
``` python
df_num = X[['AGE','HEIGHT','WEIGHT','BMI']]

df_num_prepared = num_pipeline.fit_transform(df_num)
df_num_prepared[:2].round(2)
```

::: {.output .execute_result execution_count="66"}
    array([[-0.9 ,  2.2 , -0.32, -1.19],
           [ 0.92,  0.16, -0.81, -0.91]])
:::
:::

::: {.cell .code execution_count="67"}
``` python
def monkey_patch_get_signature_names_out():
    """Monkey patch some classes which did not handle get_feature_names_out()
       correctly in Scikit-Learn 1.0.*."""
    from inspect import Signature, signature, Parameter
    import pandas as pd
    from sklearn.impute import SimpleImputer
    from sklearn.pipeline import make_pipeline, Pipeline
    from sklearn.preprocessing import FunctionTransformer, StandardScaler

    default_get_feature_names_out = StandardScaler.get_feature_names_out

    if not hasattr(SimpleImputer, "get_feature_names_out"):
      print("Monkey-patching SimpleImputer.get_feature_names_out()")
      SimpleImputer.get_feature_names_out = default_get_feature_names_out

    if not hasattr(FunctionTransformer, "get_feature_names_out"):
        print("Monkey-patching FunctionTransformer.get_feature_names_out()")
        orig_init = FunctionTransformer.__init__
        orig_sig = signature(orig_init)

        def __init__(*args, feature_names_out=None, **kwargs):
            orig_sig.bind(*args, **kwargs)
            orig_init(*args, **kwargs)
            args[0].feature_names_out = feature_names_out

        __init__.__signature__ = Signature(
            list(signature(orig_init).parameters.values()) + [
                Parameter("feature_names_out", Parameter.KEYWORD_ONLY)])

        def get_feature_names_out(self, names=None):
            if callable(self.feature_names_out):
                return self.feature_names_out(self, names)
            assert self.feature_names_out == "one-to-one"
            return default_get_feature_names_out(self, names)

        FunctionTransformer.__init__ = __init__
        FunctionTransformer.get_feature_names_out = get_feature_names_out

monkey_patch_get_signature_names_out()
```
:::

::: {.cell .code execution_count="68"}
``` python
df_num_prepared = pd.DataFrame(
    df_num_prepared, columns=num_pipeline.get_feature_names_out(),
    index=df_num.index)
```
:::

::: {.cell .code execution_count="69"}
``` python
df_num_prepared.head()  
```

::: {.output .execute_result execution_count="69"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3479</th>
      <td>-0.901508</td>
      <td>2.197982</td>
      <td>-0.316477</td>
      <td>-1.189419</td>
    </tr>
    <tr>
      <th>3191</th>
      <td>0.917287</td>
      <td>0.156825</td>
      <td>-0.813883</td>
      <td>-0.907602</td>
    </tr>
    <tr>
      <th>311</th>
      <td>-1.594383</td>
      <td>-0.426363</td>
      <td>0.390363</td>
      <td>0.636230</td>
    </tr>
    <tr>
      <th>5707</th>
      <td>-0.555071</td>
      <td>2.489576</td>
      <td>3.950741</td>
      <td>2.152453</td>
    </tr>
    <tr>
      <th>5553</th>
      <td>-1.074727</td>
      <td>-2.175926</td>
      <td>-2.096666</td>
      <td>-1.475641</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="70"}
``` python
common_columns = X.columns.intersection(df_num_prepared.columns)

X[common_columns] = df_num_prepared[common_columns]
```
:::

::: {.cell .code execution_count="71"}
``` python
if len(bmi_simil_22) == len(X):
    # Attach the array as a new column to X
    X['bmi_simil_22'] = bmi_simil_22
else:
    print("The length of the array does not match the number of rows in X.")
X.head()
```

::: {.output .execute_result execution_count="71"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SEX</th>
      <th>ELECTSURG</th>
      <th>AGE</th>
      <th>HEIGHT</th>
      <th>WEIGHT</th>
      <th>BMI</th>
      <th>HYPERMED</th>
      <th>DIALYSIS</th>
      <th>DIAB</th>
      <th>SMOKE</th>
      <th>Plastic</th>
      <th>After2014</th>
      <th>URR</th>
      <th>SSI</th>
      <th>CPT_15756</th>
      <th>CPT_15757</th>
      <th>CPT_15758</th>
      <th>CPT_15842</th>
      <th>CPT_19364</th>
      <th>CPT_19368</th>
      <th>CPT_20955</th>
      <th>CPT_20962</th>
      <th>CPT_26551</th>
      <th>WNDCLAS_1</th>
      <th>WNDCLAS_2</th>
      <th>WNDCLAS_3</th>
      <th>WNDCLAS_4</th>
      <th>ASACLAS_1</th>
      <th>ASACLAS_2</th>
      <th>ASACLAS_3</th>
      <th>ASACLAS_4</th>
      <th>DIAB_SMOKE</th>
      <th>BMI_SMOKE</th>
      <th>HYPERMED_SMOKE</th>
      <th>ELECTSURG_PLASTIC</th>
      <th>bmi_simil_22</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3479</th>
      <td>1</td>
      <td>1</td>
      <td>-0.901508</td>
      <td>2.197982</td>
      <td>-0.316477</td>
      <td>-1.189419</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>21.69968</td>
      <td>0</td>
      <td>1</td>
      <td>9.910213e-01</td>
    </tr>
    <tr>
      <th>3191</th>
      <td>1</td>
      <td>1</td>
      <td>0.917287</td>
      <td>0.156825</td>
      <td>-0.813883</td>
      <td>-0.907602</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
      <td>8.072265e-01</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1</td>
      <td>1</td>
      <td>-1.594383</td>
      <td>-0.426363</td>
      <td>0.390363</td>
      <td>0.636230</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
      <td>4.214101e-06</td>
    </tr>
    <tr>
      <th>5707</th>
      <td>0</td>
      <td>1</td>
      <td>-0.555071</td>
      <td>2.489576</td>
      <td>3.950741</td>
      <td>2.152453</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
      <td>3.505253e-19</td>
    </tr>
    <tr>
      <th>5553</th>
      <td>1</td>
      <td>1</td>
      <td>-1.074727</td>
      <td>-2.175926</td>
      <td>-2.096666</td>
      <td>-1.475641</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
      <td>6.456620e-01</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .markdown}
# Select and Train a Model
:::

::: {.cell .markdown}
## Training and Evaluating on the Training Set
:::

::: {.cell .code execution_count="72"}
``` python
from sklearn.linear_model import LinearRegression

lin_reg =  LinearRegression()
lin_reg.fit(X, df_labels)
```

::: {.output .execute_result execution_count="72"}
```{=html}
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
```
:::
:::

::: {.cell .markdown}
Let\'s try the full preprocessing pipeline on a few training instances:
:::

::: {.cell .code execution_count="73"}
``` python
df_predictions = lin_reg.predict(X)
df_predictions[:10]  # -2 = rounded to the nearest hundred
```

::: {.output .execute_result execution_count="73"}
    array([8.56462016, 7.75597073, 8.459771  , 7.43696473, 7.83345694,
           7.67363144, 7.92599642, 7.79313513, 7.73874969, 7.88360393])
:::
:::

::: {.cell .markdown}
Compare against the actual values:
:::

::: {.cell .code execution_count="74"}
``` python
df_labels.iloc[:10].values
```

::: {.output .execute_result execution_count="74"}
    array([ 8.6833, 10.9167,  6.6833,  7.8333,  8.2167,  7.95  , 10.0167,
            6.6333,  7.3167, 13.6   ])
:::
:::

::: {.cell .code execution_count="75"}
``` python
# extra code – computes the error ratios discussed in the book
error_ratios = df_predictions[:5]/ df_labels.iloc[:5].values - 1
print(", ".join([f"{100 * ratio:.1f}%" for ratio in error_ratios]))
```

::: {.output .stream .stdout}
    -1.4%, -29.0%, 26.6%, -5.1%, -4.7%
:::
:::

::: {.cell .code execution_count="76"}
``` python
from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(df_labels, df_predictions,
                              squared=False)
lin_rmse
```

::: {.output .execute_result execution_count="76"}
    2.9206384751122956
:::
:::

::: {.cell .code execution_count="77"}
``` python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(X, df_labels)
```

::: {.output .execute_result execution_count="77"}
```{=html}
<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div>
```
:::
:::

::: {.cell .code execution_count="78"}
``` python
df_predictions = tree_reg.predict(X)
tree_rmse = mean_squared_error(df_labels, df_predictions,
                              squared=False)
tree_rmse
```

::: {.output .execute_result execution_count="78"}
    0.12822680865207703
:::
:::

::: {.cell .markdown}
## Better Evaluation Using Cross-Validation
:::

::: {.cell .code execution_count="79"}
``` python
from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, X, df_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
```
:::

::: {.cell .code execution_count="80"}
``` python
pd.Series(tree_rmses).describe()
```

::: {.output .execute_result execution_count="80"}
    count    10.000000
    mean      4.253858
    std       0.145745
    min       4.039548
    25%       4.181151
    50%       4.229859
    75%       4.348009
    max       4.496936
    dtype: float64
:::
:::

::: {.cell .code execution_count="81"}
``` python
# extra code – computes the error stats for the linear model
lin_rmses = -cross_val_score(lin_reg, X, df_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
pd.Series(lin_rmses).describe()
```

::: {.output .execute_result execution_count="81"}
    count    10.000000
    mean      2.939877
    std       0.100264
    min       2.716208
    25%       2.902862
    50%       2.957521
    75%       3.009426
    max       3.046622
    dtype: float64
:::
:::

::: {.cell .markdown}
**Warning:** the following cell may take a few minutes to run:
:::

::: {.cell .code execution_count="82"}
``` python
from sklearn.ensemble import RandomForestRegressor

forest_reg =  RandomForestRegressor(random_state=42)
forest_rmses = -cross_val_score(forest_reg, X, df_labels,
                                scoring="neg_root_mean_squared_error", cv=10)
```
:::

::: {.cell .code execution_count="83"}
``` python
pd.Series(forest_rmses).describe()
```

::: {.output .execute_result execution_count="83"}
    count    10.000000
    mean      3.092698
    std       0.114642
    min       2.823709
    25%       3.064258
    50%       3.124072
    75%       3.153235
    max       3.224941
    dtype: float64
:::
:::

::: {.cell .markdown}
Let\'s compare this RMSE measured using cross-validation (the
\"validation error\") with the RMSE measured on the training set (the
\"training error\"):
:::

::: {.cell .code execution_count="84"}
``` python
forest_reg.fit(X, df_labels)
df_predictions = forest_reg.predict(X)
forest_rmse = mean_squared_error(df_labels, df_predictions,
                                 squared=False)
forest_rmse
```

::: {.output .execute_result execution_count="84"}
    1.1572546679726912
:::
:::

::: {.cell .markdown}
The training error is much lower than the validation error, which
usually means that the model has overfit the training set. Another
possible explanation may be that there\'s a mismatch between the
training data and the validation data, but it\'s not the case here,
since both came from the same dataset that we shuffled and split in two
parts.
:::

::: {.cell .markdown}
# Fine-Tune Your Model
:::

::: {.cell .markdown}
## Grid Search
:::

::: {.cell .markdown}
**Warning:** the following cell may take a few minutes to run:
:::

::: {.cell .code execution_count="86"}
``` python
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

tree_reg = DecisionTreeRegressor(random_state=42)

# Define the parameter grid to search
param_grid = [
    {'max_depth': [3, 10, None], 
     'min_samples_split': [2, 10, 20], 
     'min_samples_leaf': [1, 5, 10]}
]

# Create the GridSearchCV object
grid_search = GridSearchCV(tree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)

# Fit the model
grid_search.fit(X, df_labels)
```

::: {.output .execute_result execution_count="86"}
```{=html}
<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=42),
             param_grid=[{&#x27;max_depth&#x27;: [3, 10, None],
                          &#x27;min_samples_leaf&#x27;: [1, 5, 10],
                          &#x27;min_samples_split&#x27;: [2, 10, 20]}],
             return_train_score=True, scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=42),
             param_grid=[{&#x27;max_depth&#x27;: [3, 10, None],
                          &#x27;min_samples_leaf&#x27;: [1, 5, 10],
                          &#x27;min_samples_split&#x27;: [2, 10, 20]}],
             return_train_score=True, scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" ><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>
```
:::
:::

::: {.cell .markdown}
The best hyperparameter combination found:
:::

::: {.cell .code execution_count="87"}
``` python
grid_search.best_params_
```

::: {.output .execute_result execution_count="87"}
    {'max_depth': 3, 'min_samples_leaf': 10, 'min_samples_split': 2}
:::
:::

::: {.cell .code execution_count="88"}
``` python
grid_search.best_estimator_
```

::: {.output .execute_result execution_count="88"}
```{=html}
<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=3, min_samples_leaf=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" checked><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=3, min_samples_leaf=10, random_state=42)</pre></div></div></div></div></div>
```
:::
:::

::: {.cell .markdown}
Let\'s look at the score of each hyperparameter combination tested
during the grid search:
:::

::: {.cell .code execution_count="89"}
``` python
# Create a DataFrame from the grid search results
cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)

# Select and rename the relevant columns for easier interpretation
cv_res = cv_res[['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 
                 'split0_test_score', 'split1_test_score', 'split2_test_score', 
                 'split3_test_score', 'split4_test_score', 'mean_test_score']]

# Renaming the columns for clarity
score_cols = ['split0', 'split1', 'split2', 'split3', 'split4', 'mean_test_rmse']
cv_res.columns = ['max_depth', 'min_samples_split', 'min_samples_leaf'] + score_cols

# Convert scores to positive values and round
cv_res[score_cols] = -cv_res[score_cols]

cv_res.head()
```

::: {.output .execute_result execution_count="89"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_depth</th>
      <th>min_samples_split</th>
      <th>min_samples_leaf</th>
      <th>split0</th>
      <th>split1</th>
      <th>split2</th>
      <th>split3</th>
      <th>split4</th>
      <th>mean_test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>2</td>
      <td>10</td>
      <td>8.040100</td>
      <td>9.017291</td>
      <td>8.697263</td>
      <td>8.651476</td>
      <td>8.912794</td>
      <td>8.663785</td>
    </tr>
    <tr>
      <th>7</th>
      <td>3</td>
      <td>10</td>
      <td>10</td>
      <td>8.040100</td>
      <td>9.017291</td>
      <td>8.697263</td>
      <td>8.651476</td>
      <td>8.912794</td>
      <td>8.663785</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>20</td>
      <td>10</td>
      <td>8.040100</td>
      <td>9.017291</td>
      <td>8.697263</td>
      <td>8.651476</td>
      <td>8.912794</td>
      <td>8.663785</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>20</td>
      <td>5</td>
      <td>8.099825</td>
      <td>9.041215</td>
      <td>8.705866</td>
      <td>8.633275</td>
      <td>8.924932</td>
      <td>8.681023</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>2</td>
      <td>5</td>
      <td>8.099825</td>
      <td>9.043096</td>
      <td>8.705866</td>
      <td>8.633275</td>
      <td>8.924932</td>
      <td>8.681399</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .code execution_count="90"}
``` python
from sklearn.model_selection import cross_val_score


tree_reg = DecisionTreeRegressor(max_depth=3, min_samples_leaf=10,random_state=42)


tree_rmses = -cross_val_score(tree_reg, X, df_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
```
:::

::: {.cell .code execution_count="91"}
``` python
pd.Series(tree_rmses).describe()
```

::: {.output .execute_result execution_count="91"}
    count    10.000000
    mean      2.941132
    std       0.106676
    min       2.714336
    25%       2.893990
    50%       2.955117
    75%       3.012797
    max       3.063928
    dtype: float64
:::
:::

::: {.cell .markdown}
## Randomized Search
:::

::: {.cell .code execution_count="92"}
``` python
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
```
:::

::: {.cell .markdown}
Try 30 (`n_iter` × `cv`) random combinations of hyperparameters:
:::

::: {.cell .markdown}
**Warning:** the following cell may take a few minutes to run:
:::

::: {.cell .code execution_count="93"}
``` python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

tree_reg = DecisionTreeRegressor(random_state=42)

# Define the parameter grid to search
param_grid = [
    {'max_depth': [3, 10, None], 
     'min_samples_split': [2, 10, 20], 
     'min_samples_leaf': [1, 5, 10]}
]


rnd_search = RandomizedSearchCV(
    tree_reg, param_distributions=param_grid, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(X, df_labels)
```

::: {.output .execute_result execution_count="93"}
```{=html}
<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-7" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomizedSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=42),
                   param_distributions=[{&#x27;max_depth&#x27;: [3, 10, None],
                                         &#x27;min_samples_leaf&#x27;: [1, 5, 10],
                                         &#x27;min_samples_split&#x27;: [2, 10, 20]}],
                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-13" type="checkbox" ><label for="sk-estimator-id-13" class="sk-toggleable__label sk-toggleable__label-arrow">RandomizedSearchCV</label><div class="sk-toggleable__content"><pre>RandomizedSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=42),
                   param_distributions=[{&#x27;max_depth&#x27;: [3, 10, None],
                                         &#x27;min_samples_leaf&#x27;: [1, 5, 10],
                                         &#x27;min_samples_split&#x27;: [2, 10, 20]}],
                   random_state=42, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-14" type="checkbox" ><label for="sk-estimator-id-14" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-15" type="checkbox" ><label for="sk-estimator-id-15" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>
```
:::
:::

::: {.cell .code execution_count="94"}
``` python

# Create a DataFrame from the grid search results
cv_res = pd.DataFrame(rnd_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)

# Select and rename the relevant columns for easier interpretation
cv_res = cv_res[['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 
                 'split0_test_score', 'split1_test_score', 'split2_test_score', 'mean_test_score']]

# Renaming the columns for clarity
score_cols = ['split0', 'split1', 'split2', 'mean_test_rmse']
cv_res.columns = ['max_depth', 'min_samples_split', 'min_samples_leaf'] + score_cols

# Convert scores to positive values and round
cv_res[score_cols] = -cv_res[score_cols]

cv_res.head()
```

::: {.output .execute_result execution_count="94"}
```{=html}
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>max_depth</th>
      <th>min_samples_split</th>
      <th>min_samples_leaf</th>
      <th>split0</th>
      <th>split1</th>
      <th>split2</th>
      <th>mean_test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>20</td>
      <td>10</td>
      <td>2.905246</td>
      <td>2.958818</td>
      <td>2.995277</td>
      <td>2.953114</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>2.930383</td>
      <td>3.040676</td>
      <td>2.992286</td>
      <td>2.987782</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>3.064729</td>
      <td>3.080430</td>
      <td>3.114431</td>
      <td>3.086530</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>20</td>
      <td>10</td>
      <td>3.064729</td>
      <td>3.080430</td>
      <td>3.114431</td>
      <td>3.086530</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10</td>
      <td>20</td>
      <td>1</td>
      <td>3.174337</td>
      <td>3.095588</td>
      <td>3.090232</td>
      <td>3.120052</td>
    </tr>
  </tbody>
</table>
</div>
```
:::
:::

::: {.cell .markdown}
## Analyze the Best Models and Their Errors
:::

::: {.cell .code execution_count="95"}
``` python
# Retrieve the best estimator (i.e., the best decision tree model)
final_model = rnd_search.best_estimator_

# Extract feature importances from the decision tree model
feature_importances = final_model.feature_importances_

# Optionally, round the importances for easier interpretation
rounded_importances = feature_importances.round(2)

# Display or use the rounded importances as needed
print(rounded_importances)
```

::: {.output .stream .stdout}
    [0.   0.   0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.41 0.   0.1
     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.09 0.   0.
     0.   0.16 0.   0.   0.2  0.   0.   0.  ]
:::
:::

::: {.cell .code execution_count="96"}
``` python
sorted(zip(feature_importances,
           X.columns),
           reverse=True)
```

::: {.output .execute_result execution_count="96"}
    [(0.4073793862402924, 'After2014'),
     (0.1965747334666977, 'BMI_SMOKE'),
     (0.15751663454596024, 'ASACLAS_3'),
     (0.10048776892235753, 'SSI'),
     (0.09344527341384869, 'WNDCLAS_3'),
     (0.04459620341084347, 'AGE'),
     (0.0, 'bmi_simil_22'),
     (0.0, 'WNDCLAS_4'),
     (0.0, 'WNDCLAS_2'),
     (0.0, 'WNDCLAS_1'),
     (0.0, 'WEIGHT'),
     (0.0, 'URR'),
     (0.0, 'SMOKE'),
     (0.0, 'SEX'),
     (0.0, 'Plastic'),
     (0.0, 'HYPERMED_SMOKE'),
     (0.0, 'HYPERMED'),
     (0.0, 'HEIGHT'),
     (0.0, 'ELECTSURG_PLASTIC'),
     (0.0, 'ELECTSURG'),
     (0.0, 'DIALYSIS'),
     (0.0, 'DIAB_SMOKE'),
     (0.0, 'DIAB'),
     (0.0, 'CPT_26551'),
     (0.0, 'CPT_20962'),
     (0.0, 'CPT_20955'),
     (0.0, 'CPT_19368'),
     (0.0, 'CPT_19364'),
     (0.0, 'CPT_15842'),
     (0.0, 'CPT_15758'),
     (0.0, 'CPT_15757'),
     (0.0, 'CPT_15756'),
     (0.0, 'BMI'),
     (0.0, 'ASACLAS_4'),
     (0.0, 'ASACLAS_2'),
     (0.0, 'ASACLAS_1')]
:::
:::

::: {.cell .markdown}
## Evaluate Your System on the Test Set
:::

::: {.cell .code execution_count="97"}
``` python
X_test = strat_test_set.drop("OPTIME", axis=1)
y_test = strat_test_set["OPTIME"].copy()
```
:::

::: {.cell .code execution_count="98"}
``` python
#for column ELECTRSURG, there is only one row with value unknow, so we convert it to NO.
X_test['ELECTSURG']=X_test['ELECTSURG'].replace("Unknown","No")
# For some other columns with only binary values , convert to dummy variables.
X_test['SEX']=X_test['SEX'].replace({'female': 1, 'male': 0})
X_test['ELECTSURG']=X_test['ELECTSURG'].replace({'Yes': 1, 'No': 0})
X_test['HYPERMED']=X_test['HYPERMED'].replace({'Yes': 1, 'No': 0})
X_test['DIALYSIS']=X_test['DIALYSIS'].replace({'Yes': 1, 'No': 0})
X_test['SMOKE']=X_test['SMOKE'].replace({'Yes': 1, 'No': 0})
X_test['CPT']=X_test['CPT'].replace({42894: 26551, 26553: 26551})
#in the case for DIAB, we assign the value based on its described seriousness. 
X_test['DIAB']=X_test['DIAB'].replace({'NON-INSULIN': 1, 'No': 0, 'INSULIN': 2})

columns_to_encode = ['CPT',
                     'WNDCLAS',
                     'ASACLAS']  
X_test_encoded = pd.get_dummies(X_test, columns=columns_to_encode)

X_test = X_test_encoded.copy()
```
:::

::: {.cell .code execution_count="99"}
``` python
X_test["DIAB_SMOKE"] = X_test["DIAB"] * X_test["SMOKE"]
X_test["DIALYSIS_SMOKE"] = X_test["DIALYSIS"] * X_test["SMOKE"]
X_test["BMI_SMOKE"] = X_test["BMI"] * X_test["SMOKE"]
X_test["HYPERMED_SMOKE"] = X_test["HYPERMED"] * X_test["SMOKE"]
X_test["ELECTSURG_PLASTIC"] = X_test["ELECTSURG"] * X_test["Plastic"]
```
:::

::: {.cell .code execution_count="100"}
``` python
df_num = X_test[['AGE','HEIGHT','WEIGHT','BMI']]

df_num_prepared = num_pipeline.fit_transform(df_num)
df_num_prepared[:2].round(2)

df_num_prepared = pd.DataFrame(
    df_num_prepared, columns=num_pipeline.get_feature_names_out(),
    index=df_num.index)

common_columns = X_test.columns.intersection(df_num_prepared.columns)

X_test[common_columns] = df_num_prepared[common_columns]
```
:::

::: {.cell .code execution_count="101"}
``` python
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[22.]], gamma=0.1))
bmi_simil_22 = rbf_transformer.transform(X_test[["BMI"]])


if len(bmi_simil_22) == len(X_test):
    # Attach the array as a new column to X
    X_test['bmi_simil_22'] = bmi_simil_22
else:
    print("The length of the array does not match the number of rows in X.")
```
:::

::: {.cell .code execution_count="102"}
``` python
# Ensure X_test has the same columns in the same order as X_train
X_test_aligned = X_test.reindex(columns=X.columns, fill_value=0)

final_predictions = final_model.predict(X_test_aligned)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)
```

::: {.output .stream .stdout}
    2.9303640936326816
:::
:::

::: {.cell .markdown}
We can compute a 95% confidence interval for the test RMSE:
:::

::: {.cell .code execution_count="103"}
``` python
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))
```

::: {.output .execute_result execution_count="103"}
    array([2.80095105, 3.05429871])
:::
:::

::: {.cell .markdown}
We could compute the interval manually like this:
:::

::: {.cell .code execution_count="104"}
``` python
# extra code – shows how to compute a confidence interval for the RMSE
m = len(squared_errors)
mean = squared_errors.mean()
tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)
tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)
np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)
```

::: {.output .execute_result execution_count="104"}
    (2.800951053254681, 3.054298714889372)
:::
:::

::: {.cell .markdown}
Alternatively, we could use a z-score rather than a t-score. Since the
test set is not too small, it won\'t make a big difference:
:::

::: {.cell .code execution_count="105"}
``` python
# extra code – computes a confidence interval again using a z-score
zscore = stats.norm.ppf((1 + confidence) / 2)
zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)
np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)
```

::: {.output .execute_result execution_count="105"}
    (2.801081443080666, 3.0541791355014705)
:::
:::

::: {.cell .markdown}
## Model persistence using joblib
:::

::: {.cell .markdown}
Save the final model:
:::

::: {.cell .code execution_count="106"}
``` python
import joblib

joblib.dump(final_model, "my_patient_model.pkl")
```

::: {.output .execute_result execution_count="106"}
    ['my_patient_model.pkl']
:::
:::

::: {.cell .markdown}
Now you can deploy this model to production.
:::

::: {.cell .markdown}
## Data Dictionary
:::

::: {.cell .markdown}
SEX: The patient\'s biological sex (e.g., male, female).

CPT: Current Procedural Terminology, a set of medical codes used to
describe medical, surgical, and diagnostic services.

ELECTSURG: Elective Surgery, which is surgery that is scheduled in
advance because it does not involve a medical emergency.

AGE: The patient\'s age at the time of surgery or data collection.

HEIGHT: The patient\'s height, likely measured in inches or centimeters.

WEIGHT: The patient\'s weight, likely measured in pounds or kilograms.

BMI: Body Mass Index, a measure of body fat based on height and weight.

HYPERMED: Likely a reference to whether the patient is on medication for
hypertension (high blood pressure).

DIALYSIS: Indicates whether the patient is undergoing dialysis, which is
a treatment for kidney failure.

DIAB: Indicates whether the patient has diabetes.

SMOKE: Indicates whether the patient is a smoker.

WNDCLAS: Wound Classification, a system to describe the risk of
infection in a surgical wound.

ASACLAS: ASA Classification, a system used to assess the fitness of
patients before surgery (American Society of Anesthesiologists Physical
Status Classification).

OPTIME: Operating Time, the duration of the surgical procedure.

Plastic: This may refer to whether the surgery is a plastic surgery
procedure.

After2014: Likely indicates whether the data was collected after the
year 2014.

URR: This could refer to Urea Reduction Ratio, a measure used in
dialysis patients to gauge dialysis adequacy.

SSI: Surgical Site Infection, an infection that occurs after surgery in
the part of the body where the surgery took place.
:::
